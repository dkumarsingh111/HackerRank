{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_exploration(data):\n",
    "    print(\"First 5 Rows:\\n\", data.head())\n",
    "    print(\"Data Info:\\n\", data.info())\n",
    "    print(\"Statistical Summary:\\n\", data.describe())\n",
    "    return data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, schema):\n",
    "    for col, dtype in schema.items():\n",
    "        if col in data.columns:\n",
    "            if not data[col].dtype == dtype:\n",
    "                print(f\"Invalid data type in column {col}. Expected {dtype}, got {data[col].dtype}.\")\n",
    "    print(\"Data validation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_missing_values(data, strategy=\"mean\"):\n",
    "#     imputer = SimpleImputer(strategy=strategy)\n",
    "#     data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "#     return data_imputed\n",
    "\n",
    "def handle_missing_values(data, strategy=\"mean\"):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    non_numeric_data = data.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    numeric_data_imputed = pd.DataFrame(imputer.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "    \n",
    "    return pd.concat([numeric_data_imputed, non_numeric_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Manipulation (Feature Transformation, Deriving New Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def manipulate_data(data):\n",
    "#     # Example: Add a new column as a transformation of existing data\n",
    "#     data[\"new_feature\"] = data[\"feature1\"] * data[\"feature2\"]\n",
    "#     return data\n",
    "\n",
    "\n",
    "def manipulate_data(data):\n",
    "    if \"feature1\" in data.columns and \"feature2\" in data.columns:\n",
    "        data[\"new_feature\"] = data[\"feature1\"] * data[\"feature2\"]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def treat_outliers(data, threshold=3):\n",
    "#     z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))\n",
    "#     data_no_outliers = data[(z_scores < threshold).all(axis=1)]\n",
    "#     print(f\"Outliers removed. New data shape: {data_no_outliers.shape}\")\n",
    "#     return data_no_outliers\n",
    "\n",
    "\n",
    "def treat_outliers(data, threshold=3):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    z_scores = np.abs(stats.zscore(numeric_data))\n",
    "    data_no_outliers = data[(z_scores < threshold).all(axis=1)]\n",
    "    print(f\"Outliers removed. New data shape: {data_no_outliers.shape}\")\n",
    "    return data_no_outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "    categorical_data = pd.DataFrame(encoder.fit_transform(data.select_dtypes(include=[\"object\"])),\n",
    "                                    columns=encoder.get_feature_names_out())\n",
    "    data = data.drop(columns=data.select_dtypes(include=[\"object\"]).columns)\n",
    "    data = pd.concat([data, categorical_data], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Transformation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_data(data, scaler_type=\"standard\"):\n",
    "#     scaler = StandardScaler() if scaler_type == \"standard\" else MinMaxScaler()\n",
    "#     data_transformed = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "#     return data_transformed\n",
    "\n",
    "\n",
    "def transform_data(data, scaler_type=\"standard\"):\n",
    "    scaler = StandardScaler() if scaler_type == \"standard\" else MinMaxScaler()\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "    return pd.concat([scaled_data, data.select_dtypes(exclude=[np.number])], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(data):\n",
    "#     # Example: Dimensionality reduction using PCA\n",
    "#     pca = PCA(n_components=5)\n",
    "#     principal_components = pca.fit_transform(data)\n",
    "#     data_pca = pd.DataFrame(principal_components, columns=[f\"PCA_{i}\" for i in range(1, 6)])\n",
    "#     return pd.concat([data, data_pca], axis=1)\n",
    "\n",
    "\n",
    "def feature_engineering(data):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    pca = PCA(n_components=min(5, numeric_data.shape[1]))\n",
    "    principal_components = pca.fit_transform(numeric_data)\n",
    "    data_pca = pd.DataFrame(principal_components, columns=[f\"PCA_{i}\" for i in range(1, principal_components.shape[1] + 1)])\n",
    "    return pd.concat([data, data_pca], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_data(data):\n",
    "    data_deduped = data.drop_duplicates()\n",
    "    print(f\"Duplicates removed. New data shape: {data_deduped.shape}\")\n",
    "    return data_deduped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(data, file_path):\n",
    "    try:\n",
    "        data.to_csv(file_path, index=False)\n",
    "        print(f\"Data exported successfully to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Error Handling and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='data_cleaning.log', level=logging.INFO)\n",
    "\n",
    "def log_and_handle_errors(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {func.__name__}: {e}\")\n",
    "            print(f\"Error in {func.__name__}: {e}\")\n",
    "            return None\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data = log_and_handle_errors(ingest_data)\n",
    "initial_exploration = log_and_handle_errors(initial_exploration)\n",
    "validate_data = log_and_handle_errors(validate_data)\n",
    "handle_missing_values = log_and_handle_errors(handle_missing_values)\n",
    "manipulate_data = log_and_handle_errors(manipulate_data)\n",
    "treat_outliers = log_and_handle_errors(treat_outliers)\n",
    "encode_data = log_and_handle_errors(encode_data)\n",
    "transform_data = log_and_handle_errors(transform_data)\n",
    "feature_engineering = log_and_handle_errors(feature_engineering)\n",
    "deduplicate_data = log_and_handle_errors(deduplicate_data)\n",
    "export_data = log_and_handle_errors(export_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_pipeline(file_path, schema, export_path):\n",
    "    data = ingest_data(file_path)\n",
    "    if data is None:\n",
    "        return\n",
    "    \n",
    "    initial_exploration(data)\n",
    "    validate_data(data, schema)\n",
    "    \n",
    "    data = handle_missing_values(data)\n",
    "    data = manipulate_data(data)\n",
    "    data = treat_outliers(data)\n",
    "    data = encode_data(data)\n",
    "    data = transform_data(data, scaler_type=\"standard\")\n",
    "    data = feature_engineering(data)\n",
    "    data = deduplicate_data(data)\n",
    "    \n",
    "    export_data(data, export_path)\n",
    "    print(\"Data cleaning pipeline completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"feature1\": \"float64\",\n",
    "    \"feature2\": \"int64\",\n",
    "    \"category_feature\": \"object\"\n",
    "}\n",
    "\n",
    "data_cleaning_pipeline(\"healthcare_dataset_raw.csv\", schema, \"cleaned_healthcare_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
