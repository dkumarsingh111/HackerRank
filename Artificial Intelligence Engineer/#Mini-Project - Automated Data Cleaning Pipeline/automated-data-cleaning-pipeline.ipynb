{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = 'raw_healthcare_dataset.csv'\n",
    "clean_file_path = 'cleaned_healthcare_dataset.csv'\n",
    "# Add more variables as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Features Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combinations = {\n",
    "    (\"Age\", \"Billing Amount\"): lambda df, col1, col2: df[col1] * df[col2],\n",
    "    (\"Room Number\", \"Admission Type\"): lambda df, col1, col2: df[col1].astype(str) + \"_\" + df[col2],\n",
    "    (\"Age\", \"Room Number\", \"Billing Amount\"): lambda df, col1, col2, col3: df[col1] + df[col2] + df[col3],\n",
    "    # Add more features as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_exploration(data):\n",
    "    print(\"First 5 Rows:\\n\", data.head())\n",
    "    print(\"Data Info:\\n\", data.info())\n",
    "    print(\"Statistical Summary:\\n\", data.describe())\n",
    "    return data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, schema):\n",
    "    for col, dtype in schema.items():\n",
    "        if col in data.columns:\n",
    "            if not data[col].dtype == dtype:\n",
    "                print(f\"Invalid data type in column {col}. Expected {dtype}, got {data[col].dtype}.\")\n",
    "    print(\"Data validation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data, strategy=\"mean\"):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    non_numeric_data = data.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    numeric_data_imputed = pd.DataFrame(imputer.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "    \n",
    "    return pd.concat([numeric_data_imputed, non_numeric_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Manipulation (Feature Transformation, Deriving New Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_data(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    for columns, operation in feature_combinations.items():\n",
    "        if all(col in data.columns for col in columns):\n",
    "            new_feature_name = \"_\".join(col.lower() for col in columns) + \"_interaction\"\n",
    "            \n",
    "            # Apply the transformation and handle columns within the lambda\n",
    "            data[new_feature_name] = operation(data, *columns)\n",
    "            print(f\"Created new feature: {new_feature_name}\")\n",
    "        else:\n",
    "            missing_cols = [col for col in columns if col not in data.columns]\n",
    "            print(f\"Skipping feature {columns} due to missing columns: {missing_cols}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_outliers(data, threshold=3):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    z_scores = np.abs(stats.zscore(numeric_data))\n",
    "    data_no_outliers = data[(z_scores < threshold).all(axis=1)]\n",
    "    print(f\"Outliers removed. New data shape: {data_no_outliers.shape}\")\n",
    "    return data_no_outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "      \n",
    "    # Separate high-cardinality columns for alternative encoding strategies if needed\n",
    "    categorical_columns = data.select_dtypes(include=[\"object\"]).columns\n",
    "    high_cardinality_cols = [col for col in categorical_columns if data[col].nunique() > 50]  # Customize this threshold\n",
    "    low_cardinality_cols = [col for col in categorical_columns if data[col].nunique() <= 50]\n",
    "\n",
    "    # Use OneHotEncoder for low-cardinality columns only\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "    low_cardinality_data = pd.DataFrame(encoder.fit_transform(data[low_cardinality_cols]).toarray(), columns=encoder.get_feature_names_out())\n",
    "    \n",
    "    # Optionally, you could use frequency encoding for high-cardinality columns:\n",
    "    for col in high_cardinality_cols:\n",
    "        data[col] = data[col].map(data[col].value_counts(normalize=True))\n",
    "    \n",
    "    # Drop original categorical columns\n",
    "    data = data.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Concatenate encoded and original data\n",
    "    data = pd.concat([data.reset_index(drop=True), low_cardinality_data.reset_index(drop=True)], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Transformation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, scaler_type=\"standard\"):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    scaler = StandardScaler() if scaler_type == \"standard\" else MinMaxScaler()\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "    return pd.concat([scaled_data, data.select_dtypes(exclude=[np.number])], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    pca = PCA(n_components=min(5, numeric_data.shape[1]))\n",
    "    principal_components = pca.fit_transform(numeric_data)\n",
    "    data_pca = pd.DataFrame(principal_components, columns=[f\"PCA_{i}\" for i in range(1, principal_components.shape[1] + 1)])\n",
    "    return pd.concat([data, data_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_data(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        data_deduped = data.drop_duplicates()\n",
    "        print(f\"Duplicates removed. New data shape: {data_deduped.shape}\")\n",
    "        return data_deduped\n",
    "    except Exception as e:\n",
    "        print(f\"Duplicate record not found or Error in removing duplicate: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(data, file_path):\n",
    "    if data is None:\n",
    "        print(\"No data to export.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        data.to_csv(file_path, index=False)\n",
    "        print(f\"Data exported successfully to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Error Handling and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='data_cleaning.log', level=logging.INFO)\n",
    "\n",
    "def log_and_handle_errors(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {func.__name__}: {e}\")\n",
    "            print(f\"Error in {func.__name__}: {e}\")\n",
    "            return None\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data = log_and_handle_errors(ingest_data)\n",
    "initial_exploration = log_and_handle_errors(initial_exploration)\n",
    "validate_data = log_and_handle_errors(validate_data)\n",
    "handle_missing_values = log_and_handle_errors(handle_missing_values)\n",
    "manipulate_data = log_and_handle_errors(manipulate_data)\n",
    "treat_outliers = log_and_handle_errors(treat_outliers)\n",
    "encode_data = log_and_handle_errors(encode_data)\n",
    "transform_data = log_and_handle_errors(transform_data)\n",
    "feature_engineering = log_and_handle_errors(feature_engineering)\n",
    "deduplicate_data = log_and_handle_errors(deduplicate_data)\n",
    "export_data = log_and_handle_errors(export_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_pipeline(file_path, schema, export_path):\n",
    "    data = ingest_data(file_path)\n",
    "    if data is None:\n",
    "        return\n",
    "    \n",
    "    initial_exploration(data)\n",
    "    validate_data(data, schema)\n",
    "    \n",
    "    data = handle_missing_values(data)\n",
    "    data = manipulate_data(data) if data is not None else None\n",
    "    data = treat_outliers(data) if data is not None else None\n",
    "    data = encode_data(data) if data is not None else None\n",
    "    data = transform_data(data, scaler_type=\"standard\") if data is not None else None\n",
    "    data = feature_engineering(data) if data is not None else None\n",
    "    data = deduplicate_data(data) if data is not None else None\n",
    "    \n",
    "    export_data(data, export_path)\n",
    "    print(\"Data cleaning pipeline completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Schema:\n",
      "{'Name': 'object', 'Age': 'int64', 'Gender': 'object', 'Blood Type': 'object', 'Medical Condition': 'object', 'Date of Admission': 'object', 'Doctor': 'object', 'Hospital': 'object', 'Insurance Provider': 'object', 'Billing Amount': 'float64', 'Room Number': 'int64', 'Admission Type': 'object', 'Discharge Date': 'object', 'Medication': 'object', 'Test Results': 'object'}\n"
     ]
    }
   ],
   "source": [
    "def auto_generate_schema(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Generate schema dictionary with column name as key and data type as value\n",
    "    schema = {col: str(data[col].dtype) for col in data.columns}\n",
    "    \n",
    "    print(\"Generated Schema:\")\n",
    "    print(schema)\n",
    "    return schema\n",
    "\n",
    "# Generate schema\n",
    "schema = auto_generate_schema(raw_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run data_cleaning_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully with 55500 rows and 15 columns.\n",
      "First 5 Rows:\n",
      "             Name  Age  Gender Blood Type Medical Condition Date of Admission  \\\n",
      "0  Bobby JacksOn   30    Male         B-            Cancer        2024-01-31   \n",
      "1   LesLie TErRy   62    Male         A+           Obesity        2019-08-20   \n",
      "2    DaNnY sMitH   76  Female         A-           Obesity        2022-09-22   \n",
      "3   andrEw waTtS   28  Female         O+          Diabetes        2020-11-18   \n",
      "4  adrIENNE bEll   43  Female        AB+            Cancer        2022-09-19   \n",
      "\n",
      "             Doctor                    Hospital Insurance Provider  \\\n",
      "0     Matthew Smith             Sons and Miller         Blue Cross   \n",
      "1   Samantha Davies                     Kim Inc           Medicare   \n",
      "2  Tiffany Mitchell                    Cook PLC              Aetna   \n",
      "3       Kevin Wells  Hernandez Rogers and Vang,           Medicare   \n",
      "4    Kathleen Hanna                 White-White              Aetna   \n",
      "\n",
      "   Billing Amount  Room Number Admission Type Discharge Date   Medication  \\\n",
      "0    18856.281306          328         Urgent     2024-02-02  Paracetamol   \n",
      "1    33643.327287          265      Emergency     2019-08-26    Ibuprofen   \n",
      "2    27955.096079          205      Emergency     2022-10-07      Aspirin   \n",
      "3    37909.782410          450       Elective     2020-12-18    Ibuprofen   \n",
      "4    14238.317814          458         Urgent     2022-10-09   Penicillin   \n",
      "\n",
      "   Test Results  \n",
      "0        Normal  \n",
      "1  Inconclusive  \n",
      "2        Normal  \n",
      "3      Abnormal  \n",
      "4      Abnormal  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55500 entries, 0 to 55499\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Name                55500 non-null  object \n",
      " 1   Age                 55500 non-null  int64  \n",
      " 2   Gender              55500 non-null  object \n",
      " 3   Blood Type          55500 non-null  object \n",
      " 4   Medical Condition   55500 non-null  object \n",
      " 5   Date of Admission   55500 non-null  object \n",
      " 6   Doctor              55500 non-null  object \n",
      " 7   Hospital            55500 non-null  object \n",
      " 8   Insurance Provider  55500 non-null  object \n",
      " 9   Billing Amount      55500 non-null  float64\n",
      " 10  Room Number         55500 non-null  int64  \n",
      " 11  Admission Type      55500 non-null  object \n",
      " 12  Discharge Date      55500 non-null  object \n",
      " 13  Medication          55500 non-null  object \n",
      " 14  Test Results        55500 non-null  object \n",
      "dtypes: float64(1), int64(2), object(12)\n",
      "memory usage: 6.4+ MB\n",
      "Data Info:\n",
      " None\n",
      "Statistical Summary:\n",
      "                 Age  Billing Amount   Room Number\n",
      "count  55500.000000    55500.000000  55500.000000\n",
      "mean      51.539459    25539.316097    301.134829\n",
      "std       19.602454    14211.454431    115.243069\n",
      "min       13.000000    -2008.492140    101.000000\n",
      "25%       35.000000    13241.224652    202.000000\n",
      "50%       52.000000    25538.069376    302.000000\n",
      "75%       68.000000    37820.508436    401.000000\n",
      "max       89.000000    52764.276736    500.000000\n",
      "Data validation completed.\n",
      "Created new feature: age_billing amount_interaction\n",
      "Created new feature: room number_admission type_interaction\n",
      "Created new feature: age_room number_billing amount_interaction\n",
      "Outliers removed. New data shape: (55439, 18)\n",
      "Duplicates removed. New data shape: (54905, 35)\n",
      "Data exported successfully to cleaned_healthcare_dataset.csv.\n",
      "Data cleaning pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "data_cleaning_pipeline(raw_file_path, schema, clean_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
